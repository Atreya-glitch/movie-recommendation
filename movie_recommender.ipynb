{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fc1827",
   "metadata": {},
   "source": [
    "\n",
    "# Movie Recommendation System — step-by-step notebook\n",
    "**What you'll get:** a complete starter notebook to build a movie recommender (popularity baseline, collaborative filtering, content-based, and a simple classification approach).  \n",
    "**Dataset:** `Movie Recommendation System` (you provided the Kaggle dataset link).\n",
    "\n",
    "> ⚠️ I couldn't download the dataset from here (no internet in the notebook-creation environment). There's a ready-to-run cell below with Kaggle CLI commands — run them on your machine to fetch the data into `./data/` before executing the rest of the notebook.\n",
    "\n",
    "**Sections**\n",
    "1. Setup & data download\n",
    "2. Smart data loading (auto-detect CSVs)\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Popularity-based recommender (baseline)\n",
    "5. Collaborative Filtering (Surprise SVD)\n",
    "6. Content-based recommender (TF-IDF on titles)\n",
    "7. Classification: predict if a user will *like* a movie (rating >= 4)\n",
    "8. Advanced: sentence-transformers embeddings & next steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e287af",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/atreya sharma/AppData/Local/Microsoft/WindowsApps/python3.10.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 0) Setup: pip installs (run once in your env)\n",
    "# -----------------------------\n",
    "# Uncomment and run these if you don't have the libraries.\n",
    "# Note: sentence-transformers is optional (used in advanced section).\n",
    "#\n",
    "# !pip install pandas numpy scikit-learn matplotlib scikit-surprise sentence-transformers\n",
    "#\n",
    "# If you prefer conda:\n",
    "# conda install -c conda-forge scikit-surprise pandas scikit-learn matplotlib\n",
    "#\n",
    "print('Skip installs if already available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d7bc2",
   "metadata": {},
   "source": [
    "\n",
    "## Download dataset (two options)\n",
    "\n",
    "**Option A — Kaggle CLI (recommended if the dataset is on Kaggle)**\n",
    "\n",
    "1. Install kaggle CLI and configure an API token: https://github.com/Kaggle/kaggle-api\n",
    "2. Run in terminal (not inside this notebook necessarily):\n",
    "```bash\n",
    "kaggle datasets download -d parasharmanas/movie-recommendation-system -p ./data\n",
    "unzip ./data/movie-recommendation-system.zip -d ./data\n",
    "```\n",
    "\n",
    "**Option B — manual**\n",
    "- Go to the Kaggle dataset page (the link you provided) and download `.csv` files. Put them in `./data/`.\n",
    "- The notebook will auto-detect CSVs in `./data/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = './data'\n",
    "os.makedirs(data_dir, exist_ok=True)  # create if missing\n",
    "\n",
    "print('Files in', data_dir, ':', os.listdir(data_dir))\n",
    "\n",
    "csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "print('\\nDetected CSV files:', [os.path.basename(f) for f in csv_files])\n",
    "\n",
    "# try to read all CSVs into memory (careful with very large files)\n",
    "dfs = {}\n",
    "for p in csv_files:\n",
    "    name = os.path.basename(p)\n",
    "    try:\n",
    "        dfs[name] = pd.read_csv(p, low_memory=False)\n",
    "        print(f'Loaded {name} — shape={dfs[name].shape}')\n",
    "    except Exception as e:\n",
    "        print('Failed to read', name, e)\n",
    "\n",
    "# helper to guess which dataframe is which based on column heuristics\n",
    "def find_df_by_cols(dfs, required_cols_any):\n",
    "    for name, df in dfs.items():\n",
    "        cols = [c.lower() for c in df.columns]\n",
    "        # fix: reversed loop order and condition\n",
    "        if any(req in c for req in required_cols_any for c in cols):\n",
    "            return name, df\n",
    "    return None, None\n",
    "\n",
    "# Common heuristics\n",
    "ratings_name, ratings_df = find_df_by_cols(dfs, ['rating', 'user', 'movie'])\n",
    "movies_name, movies_df = find_df_by_cols(dfs, ['title', 'genres', 'movie'])\n",
    "\n",
    "print('\\nGuessed ratings file:', ratings_name)\n",
    "print('Guessed movies file:', movies_name)\n",
    "\n",
    "# Show samples (if found)\n",
    "if ratings_df is not None:\n",
    "    print('\\nRatings sample:')\n",
    "    display(ratings_df.head())\n",
    "else:\n",
    "    print('\\nNo ratings file auto-detected. If your dataset uses different column names, inspect ./data and load manually.')\n",
    "\n",
    "if movies_df is not None:\n",
    "    print('\\nMovies sample:')\n",
    "    display(movies_df.head())\n",
    "else:\n",
    "    print('\\nNo movies file auto-detected. If your dataset uses different column names, inspect ./data and load manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d579a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 2) Quick EDA (ratings distribution, counts)\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'ratings_df' in globals() and ratings_df is not None:\n",
    "    # try to find rating column name\n",
    "    rating_col = None\n",
    "    for c in ratings_df.columns:\n",
    "        if 'rating' in c.lower():\n",
    "            rating_col = c\n",
    "            break\n",
    "    print('rating_col =', rating_col)\n",
    "    print('\\nBasic stats:')\n",
    "    display(ratings_df[rating_col].describe())\n",
    "    \n",
    "    # histogram\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(ratings_df[rating_col].dropna(), bins=20)\n",
    "    plt.title('Rating distribution')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    # unique counts\n",
    "    user_cols = [c for c in ratings_df.columns if 'user' in c.lower()]\n",
    "    movie_cols = [c for c in ratings_df.columns if 'movie' in c.lower() or 'title' in c.lower()]\n",
    "    print('\\nUnique users (guess):', ratings_df[user_cols[0]].nunique() if user_cols else 'N/A')\n",
    "    print('Unique movies (guess):', ratings_df[movie_cols[0]].nunique() if movie_cols else 'N/A')\n",
    "else:\n",
    "    print('No ratings_df available for EDA. Load your CSVs into ./data/ and re-run the loader cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 3) Popularity-based recommender (baseline)\n",
    "# -----------------------------\n",
    "# This recommends overall top movies by average rating and minimum number of ratings.\n",
    "if 'ratings_df' in globals() and ratings_df is not None:\n",
    "    # find column names\n",
    "    def find_col(df, options):\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if c.lower() == o.lower():\n",
    "                    return c\n",
    "        # partial match\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if o.lower() in c.lower():\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    user_col = find_col(ratings_df, ['userId', 'user_id', 'user'])\n",
    "    movie_col = find_col(ratings_df, ['movieId', 'movie_id', 'movie'])\n",
    "    rating_col = find_col(ratings_df, ['rating'])\n",
    "\n",
    "    agg = ratings_df.groupby(movie_col)[rating_col].agg(['mean', 'count']).reset_index().rename(columns={'mean': 'avg_rating', 'count': 'n_ratings'})\n",
    "    display(agg.sort_values(['avg_rating', 'n_ratings'], ascending=[False, False]).head(20))\n",
    "\n",
    "    # join titles if movies_df exists\n",
    "    if 'movies_df' in globals() and movies_df is not None:\n",
    "        title_col = find_col(movies_df, ['title', 'name'])\n",
    "        if title_col is not None:\n",
    "            agg = agg.merge(movies_df[[movie_col, title_col]].drop_duplicates(), on=movie_col, how='left')\n",
    "            print('\\nTop movies (with titles):')\n",
    "            display(agg.sort_values(['avg_rating', 'n_ratings'], ascending=[False, False]).head(20))\n",
    "else:\n",
    "    print('ratings_df not available — run the loader cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 4) Collaborative Filtering using Surprise (SVD)\n",
    "# -----------------------------\n",
    "# Trains SVD on user-item ratings and reports RMSE. Then shows how to get top-N recommendations for a user.\n",
    "try:\n",
    "    from surprise import Dataset, Reader, SVD\n",
    "    from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "    from surprise import accuracy\n",
    "except Exception as e:\n",
    "    print('surprise library not installed; install with: pip install scikit-surprise')\n",
    "    raise e\n",
    "\n",
    "if 'ratings_df' in globals() and ratings_df is not None:\n",
    "    # find columns\n",
    "    def find_col(df, options):\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if c.lower() == o.lower():\n",
    "                    return c\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if o.lower() in c.lower():\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    user_col = find_col(ratings_df, ['userId', 'user_id', 'user'])\n",
    "    movie_col = find_col(ratings_df, ['movieId', 'movie_id', 'movie'])\n",
    "    rating_col = find_col(ratings_df, ['rating'])\n",
    "\n",
    "    print('Using columns:', user_col, movie_col, rating_col)\n",
    "    df_sub = ratings_df[[user_col, movie_col, rating_col]].dropna()\n",
    "\n",
    "    # ensure rating range defined\n",
    "    minr, maxr = df_sub[rating_col].min(), df_sub[rating_col].max()\n",
    "    reader = Reader(rating_scale=(minr, maxr))\n",
    "    data = Dataset.load_from_df(df_sub[[user_col, movie_col, rating_col]], reader)\n",
    "\n",
    "    trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    algo = SVD(n_factors=50, random_state=42)\n",
    "    algo.fit(trainset)\n",
    "    preds = algo.test(testset)\n",
    "\n",
    "    print('\\nRMSE on testset:')\n",
    "    accuracy.rmse(preds, verbose=True)\n",
    "\n",
    "    # helper: get top-N recommendations for a raw user id\n",
    "    def get_top_n_recommendations(algo, user_raw_id, movies_df=None, n=10):\n",
    "        # collect all movie ids\n",
    "        all_movie_ids = df_sub[movie_col].unique()\n",
    "        # movies the user has rated\n",
    "        if user_raw_id in df_sub[user_col].values:\n",
    "            user_rated = df_sub[df_sub[user_col] == user_raw_id][movie_col].unique()\n",
    "        else:\n",
    "            user_rated = []\n",
    "        candidates = [m for m in all_movie_ids if m not in user_rated]\n",
    "        predictions = [(m, algo.predict(user_raw_id, m).est) for m in candidates]\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        topn = predictions[:n]\n",
    "\n",
    "        if movies_df is not None:\n",
    "            title_col = find_col(movies_df, ['title', 'name'])\n",
    "            rows = []\n",
    "            for mid, score in topn:\n",
    "                title = movies_df.loc[movies_df[movie_col] == mid, title_col].values\n",
    "                rows.append({\n",
    "                    'movieId': mid,\n",
    "                    'pred_score': score,\n",
    "                    'title': title[0] if len(title) > 0 else None\n",
    "                })\n",
    "            return rows\n",
    "        else:\n",
    "            return [{'movieId': m, 'pred_score': s} for m, s in topn]\n",
    "\n",
    "    # Example: replace with a real user id from your data\n",
    "    example_user = df_sub[user_col].iloc[0]\n",
    "    print('\\nTop 10 recommendations for user', example_user)\n",
    "    display(get_top_n_recommendations(algo, example_user, movies_df=globals().get('movies_df', None), n=10))\n",
    "\n",
    "else:\n",
    "    print('ratings_df required — run the loader cell to populate ratings_df.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53642a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5) Content-based recommender using TF-IDF on titles\n",
    "# -----------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd  # missing import for pd.Series\n",
    "\n",
    "# We'll use the movies_df if it exists; otherwise try to merge movie titles from ratings_df if present.\n",
    "if 'movies_df' in globals() and movies_df is not None:\n",
    "    title_col = None\n",
    "    for c in movies_df.columns:\n",
    "        if 'title' in c.lower() or 'name' in c.lower():\n",
    "            title_col = c\n",
    "            break\n",
    "    movies_for_cb = movies_df.copy()\n",
    "    movies_for_cb['title_text'] = movies_for_cb[title_col].astype(str)\n",
    "elif 'ratings_df' in globals() and ratings_df is not None:\n",
    "    # try to find a title-like column in ratings table\n",
    "    title_col = None\n",
    "    for c in ratings_df.columns:\n",
    "        if 'title' in c.lower() or 'name' in c.lower():\n",
    "            title_col = c\n",
    "            break\n",
    "    if title_col is not None:\n",
    "        movies_for_cb = ratings_df[[title_col]].drop_duplicates().rename(columns={title_col: 'title_text'})\n",
    "    else:\n",
    "        movies_for_cb = None\n",
    "else:\n",
    "    movies_for_cb = None\n",
    "\n",
    "if movies_for_cb is None:\n",
    "    print('No title text found in your CSVs. Content-based recommender requires a title/description column.')\n",
    "else:\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(movies_for_cb['title_text'].fillna(''))\n",
    "\n",
    "    # cosine similarities\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    indices = pd.Series(movies_for_cb.index, index=movies_for_cb['title_text']).drop_duplicates()\n",
    "\n",
    "    # helper to recommend by movie title string (fuzzy match)\n",
    "    from difflib import get_close_matches\n",
    "\n",
    "    def find_title_match(query, choices, n=1):\n",
    "        matches = get_close_matches(query, choices, n=n, cutoff=0.4)\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    def recommend_by_title(query_title, topn=10):\n",
    "        match = find_title_match(query_title, movies_for_cb['title_text'].tolist(), n=1)\n",
    "        if match is None:\n",
    "            print('No close title match found for query:', query_title)\n",
    "            return []\n",
    "        idx = indices[match]\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:topn + 1]  # exclude the movie itself\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "        return movies_for_cb.iloc[movie_indices][['title_text']].assign(score=[i[1] for i in sim_scores])\n",
    "\n",
    "    # small demo (replace with a title from your dataset)\n",
    "    sample_title = movies_for_cb['title_text'].iloc[0]\n",
    "    print('Sample title:', sample_title)\n",
    "    display(recommend_by_title(sample_title, topn=8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ace3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # missing import added\n",
    "import pandas as pd  # required for merge operations\n",
    "from scipy.sparse import hstack  # moved to top for clarity\n",
    "\n",
    "if 'ratings_df' not in globals() or ratings_df is None:\n",
    "    print('ratings_df required — run the loader cell first.')\n",
    "else:\n",
    "    # find column names robustly\n",
    "    def find_col(df, options):\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if c.lower() == o.lower():\n",
    "                    return c\n",
    "        for o in options:\n",
    "            for c in df.columns:\n",
    "                if o.lower() in c.lower():\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    user_col = find_col(ratings_df, ['userId', 'user_id', 'user'])\n",
    "    movie_col = find_col(ratings_df, ['movieId', 'movie_id', 'movie'])\n",
    "    rating_col = find_col(ratings_df, ['rating'])\n",
    "\n",
    "    # Prepare dataset\n",
    "    dfc = ratings_df[[user_col, movie_col, rating_col]].copy().dropna()\n",
    "    dfc['liked'] = (dfc[rating_col] >= 4).astype(int)  # binary target (tweak threshold as needed)\n",
    "\n",
    "    # aggregate movie stats\n",
    "    movie_stats = dfc.groupby(movie_col)[rating_col].agg(['mean', 'count']).reset_index().rename(\n",
    "        columns={'mean': 'movie_avg_rating', 'count': 'movie_rating_count'}\n",
    "    )\n",
    "    user_stats = dfc.groupby(user_col)[rating_col].agg(['mean']).reset_index().rename(\n",
    "        columns={'mean': 'user_avg_rating'}\n",
    "    )\n",
    "\n",
    "    dfc = dfc.merge(movie_stats, on=movie_col, how='left').merge(user_stats, on=user_col, how='left')\n",
    "\n",
    "    # attach movie title if movies_df is available (for TF-IDF)\n",
    "    if 'movies_df' in globals() and movies_df is not None:\n",
    "        title_col = None\n",
    "        for c in movies_df.columns:\n",
    "            if 'title' in c.lower() or 'name' in c.lower():\n",
    "                title_col = c\n",
    "                break\n",
    "        if title_col is not None:\n",
    "            dfc = dfc.merge(movies_df[[movie_col, title_col]].drop_duplicates(), on=movie_col, how='left')\n",
    "            dfc['title_text'] = dfc[title_col].astype(str)\n",
    "        else:\n",
    "            dfc['title_text'] = ''\n",
    "    else:\n",
    "        dfc['title_text'] = ''\n",
    "\n",
    "    # TF-IDF on title_text\n",
    "    tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english')\n",
    "    X_text = tfidf.fit_transform(dfc['title_text'].fillna(''))\n",
    "\n",
    "    # numeric features\n",
    "    num_feats = ['movie_avg_rating', 'movie_rating_count', 'user_avg_rating']\n",
    "    X_num = dfc[num_feats].fillna(0).values\n",
    "\n",
    "    # combine sparse + dense\n",
    "    X = hstack([X_text, X_num])\n",
    "    y = dfc['liked'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # logistic regression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('\\nLogistic Regression results:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # random forest on numeric only (as an alternative)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train[:, -len(num_feats):].toarray(), y_train)\n",
    "    y_pred_rf = rf.predict(X_test[:, -len(num_feats):].toarray())\n",
    "    print('\\nRandom Forest (numeric-only) results:')\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    print('accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "    print('\\nNote: Text features often help but require enough data & tuning.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525ce31",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced (optional): use sentence-transformers embeddings for better text vectors\n",
    "\n",
    "If you want stronger semantic embeddings (title/description), use `sentence-transformers`:\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # small and fast\n",
    "embs = model.encode(list_of_titles, show_progress_bar=True, convert_to_numpy=True)\n",
    "```\n",
    "Then you can index embeddings with Annoy / Faiss / Milvus for fast nearest-neighbor search, or use cosine similarity directly for small datasets.\n",
    "\n",
    "**Tip:** TF-IDF is cheap and often works surprisingly well for short title text. For descriptions or plots, embeddings shine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2ffe8",
   "metadata": {},
   "source": [
    "\n",
    "## Utilities & Next steps\n",
    "\n",
    "**Ideas to improve & expand**\n",
    "- Use full movie metadata (plot, genres, cast) for content-based models.\n",
    "- Use matrix-factorization (ALS) at scale, or neural CF models for deep learning approaches.\n",
    "- Hybrid: combine content-similarity features with CF predictions as inputs for a classifier or ranker.\n",
    "- For production: expose the model via a small Flask/FastAPI service and cache recommendations for speed.\n",
    "\n",
    "**How to run**\n",
    "1. Download the dataset into `./data/` using the Kaggle CLI or manually.\n",
    "2. Open this notebook and run cells top-to-bottom.\n",
    "3. Tweak thresholds, hyperparameters, and vector sizes based on results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
